{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D, Bidirectional\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk import ngrams\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF0AzKPH10gw",
        "outputId": "11d912ac-a4ad-417b-a0e7-544a4ee0a0d2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "3kKKNSBeiwtd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "train = fetch_20newsgroups(subset='train')\n",
        "test = fetch_20newsgroups(subset='test')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex = 1000\n",
        "print(train[\"data\"][ex])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g89ezHSGz-yw",
        "outputId": "35d7e058-ff63-4a7a-d7d1-03383ea72a15"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\n",
            "Subject: Diamond SS24X, Win 3.1, Mouse cursor\n",
            "Organization: National Library of Medicine\n",
            "Lines: 10\n",
            "\n",
            "\n",
            "Anybody seen mouse cursor distortion running the Diamond 1024x768x256 driver?\n",
            "Sorry, don't know the version of the driver (no indication in the menus) but it's a recently\n",
            "delivered Gateway system.  Am going to try the latest drivers from Diamond BBS but wondered\n",
            "if anyone else had seen this.\n",
            "\n",
            "post or email\n",
            "\n",
            "--Don Lindbergh\n",
            "dabl2@lhc.nlm.nih.gov\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_nb = 20000\n",
        "\n",
        "df_train = train[\"data\"]\n",
        "df_test = test[\"data\"]\n",
        "\n",
        "# vectorize the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(nb_words=max_nb, char_level=False)\n",
        "tokenizer.fit_on_texts(df_train)\n",
        "sequences = tokenizer.texts_to_sequences(df_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_CzcLuE3HPm",
        "outputId": "9114550e-2007-42c0-81f2-e120550a40ed"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/preprocessing/text.py:234: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
            "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "maxlength = 1000\n",
        "#pad sequence\n",
        "x_train = pad_sequences(sequences, maxlen=maxlength)\n",
        "x_test = pad_sequences(sequences_test, maxlen=maxlength)\n",
        "print('Shape of data tensor:', x_train.shape)\n",
        "print('Shape of data test tensor:', x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NygeAHuD3ZbY",
        "outputId": "9d84180d-c535-4faf-e53f-3e150bebda81"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor: (11314, 1000)\n",
            "Shape of data test tensor: (7532, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "y_train = train[\"target\"]\n",
        "y_test = test[\"target\"]\n",
        "\n",
        "y_train = to_categorical(np.asarray(y_train))\n",
        "print('Shape of label tensor:', y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjZfNBPo3wf2",
        "outputId": "c9000149-4e19-44fa-fc41-6c6fc4362cf3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of label tensor: (11314, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 50"
      ],
      "metadata": {
        "id": "oh-AqycU4NpH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape=(maxlength,), dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(max_nb, embedding_dim,\n",
        "                            input_length=maxlength,\n",
        "                            trainable=True)\n",
        "embedded_sequences = embedding_layer(sequence_input)"
      ],
      "metadata": {
        "id": "rsJM4yxH4EgY"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM, Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "sequence_input = Input(shape=(maxlength,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "# 1D convolution with 64 output channels\n",
        "x = Conv1D(64, 5)(embedded_sequences)\n",
        "#divide by 5 \n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Conv1D(64, 5)(x)\n",
        "x = MaxPooling1D(5)(x)\n",
        "# LSTM\n",
        "x = LSTM(64)(x)\n",
        "predictions = Dense(20, activation='softmax')(x)\n",
        "\n",
        "model = Model(sequence_input, predictions)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "Wk31qBgr32uS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "\n",
        "sequence_input = Input(shape=(maxlength,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "# A 1D convolution with 128 output channels\n",
        "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "x = MaxPooling1D(5)(x)\n",
        "# A 1D convolution with 64 output channels\n",
        "x = Conv1D(64, 5, activation='relu')(x)\n",
        "# MaxPool divides the length of the sequence by 5\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Flatten()(x)\n",
        "\n",
        "predictions = Dense(20, activation='softmax')(x)\n",
        "\n",
        "model = Model(sequence_input, predictions)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "owZC7vsM4dg5"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vzYSlKLmATKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "high batch -> can cause exhausted resource error that can cause by running out of memory.\n",
        "low batch -> slower training speed"
      ],
      "metadata": {
        "id": "4V8nqXuxATk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, validation_split=0.1,\n",
        "          epochs=10, batch_size=128, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HDDzvMu4Xj-",
        "outputId": "3ea46ee9-8379-46c9-d5a1-0e27710905f9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "80/80 - 80s - loss: 2.9391 - acc: 0.0757 - val_loss: 2.8160 - val_acc: 0.1131 - 80s/epoch - 1s/step\n",
            "Epoch 2/10\n",
            "80/80 - 80s - loss: 2.3979 - acc: 0.1701 - val_loss: 2.1768 - val_acc: 0.2173 - 80s/epoch - 997ms/step\n",
            "Epoch 3/10\n",
            "80/80 - 76s - loss: 1.8478 - acc: 0.3360 - val_loss: 1.9270 - val_acc: 0.3322 - 76s/epoch - 953ms/step\n",
            "Epoch 4/10\n",
            "80/80 - 88s - loss: 1.2116 - acc: 0.5857 - val_loss: 1.4626 - val_acc: 0.5415 - 88s/epoch - 1s/step\n",
            "Epoch 5/10\n",
            "80/80 - 75s - loss: 0.6195 - acc: 0.7935 - val_loss: 1.3816 - val_acc: 0.6299 - 75s/epoch - 933ms/step\n",
            "Epoch 6/10\n",
            "80/80 - 83s - loss: 0.3031 - acc: 0.9135 - val_loss: 1.4549 - val_acc: 0.6643 - 83s/epoch - 1s/step\n",
            "Epoch 7/10\n",
            "80/80 - 75s - loss: 0.1614 - acc: 0.9593 - val_loss: 1.6511 - val_acc: 0.6837 - 75s/epoch - 934ms/step\n",
            "Epoch 8/10\n",
            "80/80 - 87s - loss: 0.0860 - acc: 0.9837 - val_loss: 1.7508 - val_acc: 0.6952 - 87s/epoch - 1s/step\n",
            "Epoch 9/10\n",
            "80/80 - 94s - loss: 0.0583 - acc: 0.9887 - val_loss: 1.9039 - val_acc: 0.6890 - 94s/epoch - 1s/step\n",
            "Epoch 10/10\n",
            "80/80 - 75s - loss: 0.0373 - acc: 0.9940 - val_loss: 1.9802 - val_acc: 0.7005 - 75s/epoch - 939ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1ba6a454f0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "early_stop = EarlyStopping(monitor = 'val_loss',\n",
        "                           patience = 2)\n",
        "\n",
        "history = model.fit(x_train, y_train, \n",
        "                    validation_split=0.1,\n",
        "                    epochs=10, \n",
        "                    batch_size=32,\n",
        "                    callbacks = [early_stop],\n",
        "                    verbose = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txGawrBsChPc",
        "outputId": "192e9ac1-95de-4e85-d19e-764bc5e9de30"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "319/319 - 88s - loss: 0.0056 - acc: 0.9992 - val_loss: 2.2836 - val_acc: 0.7341 - 88s/epoch - 276ms/step\n",
            "Epoch 2/10\n",
            "319/319 - 86s - loss: 0.0278 - acc: 0.9943 - val_loss: 2.6787 - val_acc: 0.7120 - 86s/epoch - 269ms/step\n",
            "Epoch 3/10\n",
            "319/319 - 85s - loss: 0.0776 - acc: 0.9805 - val_loss: 2.3742 - val_acc: 0.7465 - 85s/epoch - 267ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('lstm')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSQPmdubNvYA",
        "outputId": "09ce9dc6-8fbc-4618-ae5e-dfc7e30def07"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ]
}