{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "print(env.observation_space.low,\"\\n\",env.observation_space.high)\n",
    "def Qtable(state_space,action_space,bin_size = 30):\n",
    "    bins = [np.linspace(-4.8,4.8,bin_size),\n",
    "            np.linspace(-4,4,bin_size),\n",
    "            np.linspace(-0.418,0.418,bin_size),\n",
    "            np.linspace(-4,4,bin_size)]\n",
    "    \n",
    "    q_table = np.random.uniform(low=-1,high=1,size=([bin_size] * state_space + [action_space]))\n",
    "    return q_table, bins\n",
    "\n",
    "# discretize the states so that gym can read it\n",
    "def Discrete(state, bins):\n",
    "    index = []\n",
    "    for i in range(len(state)): \n",
    "        index.append(np.digitize(state[i],bins[i]) - 1)\n",
    "    return tuple(index)\n",
    "\n",
    "\n",
    "'''\n",
    "======\n",
    "epsilon is associated with how random you take an action.\n",
    "The epsilon refers to the Exploration vs. Exploitation problem.\n",
    "Exploration allows the agent to improve current knowledge about the environment by choosing random action.\n",
    "On the other hand, Exploitation determines the “greedy” action to get the most reward. So the greedy action is picked with the probability of 1-ϵ and random action with ϵ.\n",
    "gamma = Discount Factor\n",
    "lr = alpha or learning rate is associated with how big you take a leap\n",
    "timestep = how often do we want to print the result on the screen\n",
    "=====\n",
    "'''\n",
    "\n",
    "def Q_learning(q_table, bins, episodes = 5000, gamma = 0.95, lr = 0.1, timestep = 5000, epsilon = 0.2):\n",
    "    rewards = 0\n",
    "    solved = False \n",
    "    steps = 0 # for learning rate\n",
    "    runs = [0]\n",
    "    data = {'max' : [0], 'avg' : [0]}\n",
    "    start = time.time()\n",
    "    ep = [i for i in range(0,episodes + 1,timestep)] \n",
    "    \n",
    "    #iterating through n episodes\n",
    "    for episode in range(1,episodes+1):\n",
    "        \n",
    "        current_state = Discrete(env.reset()[0], bins) # initial observation\n",
    "        score = 0\n",
    "        done = False\n",
    "        temp_start = time.time()\n",
    "        \n",
    "        while not done:\n",
    "            steps += 1 \n",
    "            ep_start = time.time()\n",
    "            if episode%timestep == 0:\n",
    "                env.render() # to render the result on the screen\n",
    "                \n",
    "            \n",
    "            if np.random.uniform(0,1) < epsilon:\n",
    "                #exploration: random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                #exploitation: finding the largest reward\n",
    "                action = np.argmax(q_table[current_state])\n",
    "            \n",
    "            observation, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            next_state = Discrete(observation,bins)\n",
    "            # print(observation,next_state)\n",
    "            # sleep(2)\n",
    "\n",
    "            score += reward\n",
    "            \n",
    "            # Learning part, updating the q value\n",
    "            if not done:\n",
    "                max_future_q = np.max(q_table[next_state])\n",
    "                current_q = q_table[current_state+(action,)]\n",
    "                new_q = (1-lr)*current_q + lr*(reward + gamma*max_future_q)\n",
    "                q_table[current_state+(action,)] = new_q\n",
    "\n",
    "            current_state = next_state\n",
    "            \n",
    "        # End of the loop update\n",
    "        else:\n",
    "            rewards += score\n",
    "            # print(score)\n",
    "            runs.append(score)\n",
    "            if score > 195 and steps >= 100 and solved == False: # considered as a solved:\n",
    "                solved = True\n",
    "                print('Solved in episode : {} in time {}'.format(episode, (time.time()-ep_start)))\n",
    "        \n",
    "        # Timestep value update\n",
    "        if episode%timestep == 0:\n",
    "            print('Episode : {} | Reward -> {} | Max reward : {} | Time : {}'.format(episode,rewards/timestep, max(runs), time.time() - ep_start))\n",
    "            data['max'].append(max(runs))\n",
    "            data['avg'].append(rewards/timestep)\n",
    "            if rewards/timestep >= 195: \n",
    "                print('Solved in episode : {}'.format(episode))\n",
    "            rewards, runs= 0, [0] \n",
    "            \n",
    "    #Let's see the result in a line plot\n",
    "    if len(ep) == len(data['max']):\n",
    "        plt.plot(ep, data['max'], label = 'Max')\n",
    "        plt.plot(ep, data['avg'], label = 'Avg')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend(loc = \"upper left\")\n",
    "        plt.show()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# TRANING\n",
    "q_table, bins = Qtable(len(env.observation_space.low), env.action_space.n)\n",
    "\n",
    "Q_learning(q_table, bins, lr = 0.15, gamma = 0.995, episodes = 5*10**3, timestep = 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
